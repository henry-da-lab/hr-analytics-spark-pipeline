# HR Analytics Data Pipeline using Apache Spark

![Python](https://img.shields.io/badge/Python-3.x-blue)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-Distributed-orange)
![PySpark](https://img.shields.io/badge/PySpark-ETL-yellow)
![Data Engineering](https://img.shields.io/badge/Data%20Engineering-Big%20Data-green)
![Scale](https://img.shields.io/badge/Scale-3M%2B%20Rows-red)

---

## Project Overview

This project implements a **production-style HR analytics data pipeline** using **Apache Spark (PySpark)** to integrate **heterogeneous HR and Payroll datasets** into unified, analytics-ready outputs.

The pipeline processes **~3 million records** resulting from complex many-to-many joins and produces structured analytical tables similar to those generated by enterprise HR platforms such as **Workday HCM**, **BambooHR**, and **Sage People**.

Unlike proprietary systems, this solution is **fully transparent, reproducible, and cloud-ready**, demonstrating real-world **data engineering practices**.

---

## Business Problem

In most organisations:
- HR and Payroll systems operate in **separate silos**
- Schemas and formats are incompatible
- No shared employee identifiers exist

This fragmentation prevents unified analysis of:
- Demographics vs compensation
- Engagement vs turnover
- Workforce structure across departments

This pipeline solves that problem using **distributed data processing** and **domain-based schema harmonisation**.

---

## Why Apache Spark?

Apache Spark was selected over MapReduce, Hive, and Storm because it provides:

- In-memory distributed processing for large-scale analytics
- A unified API (DataFrames, SQL, functional transformations)
- Efficient handling of joins and aggregations
- Seamless scalability from local development to cloud clusters
- Strong industry adoption in modern data engineering teams

---

## Data Sources

- **HR Analytics Dataset (Kaggle)**  
  ~3,150 employees, 40+ HR attributes  
  https://www.kaggle.com/datasets/hopesb/hr-analytics-dataset/data

- **City of Houston Payroll Dataset**  
  20,000+ employees, compensation and job data  
  https://data.houstontx.gov/dataset/payroll

The datasets have **no shared identifiers**, reflecting real enterprise integration challenges.

---

## Pipeline Architecture

**ETL Flow:**

*(See full report for detailed pipeline diagrams and screenshots.)*

---

## Key Technical Features

- Schema harmonisation using **department-level domain mapping**
- Intelligent handling of business-meaningful null values
- Date parsing and normalization across inconsistent formats
- Feature engineering (age, tenure, pay grade categories)
- Distributed many-to-many joins producing **3M+ rows**
- Aggregation strategies to control join explosion
- **Pure Spark implementation** (no Pandas or local processing)

---

## Analytical Outputs

The pipeline generates five production-ready analytical tables:

- Workforce demographics (age, gender, race)
- Department-level engagement metrics
- Compensation summaries (base, overtime, total)
- Turnover rate analysis
- Training outcomes assessment

Outputs are exported as CSV files and are ready for BI tools such as **Tableau**, **Power BI**, or **Excel**.

---

## Engineering Challenges Solved

- Spark & JVM setup on Windows
- Schema misalignment between independent datasets
- Managing join explosion from many-to-many relationships
- Memory optimisation during large transformations
- Implementing all logic using native Spark APIs

---

## How to Run (Local)

**Requirements**
- Windows 11
- Python 3.x
- Java JDK 21
- Apache Spark
- Hadoop WinUtils

Run the pipeline via **PySpark in Jupyter Notebook**.  
All transformations are executed within Spark.

*(Detailed setup instructions and troubleshooting are documented in the report.)*

---

## Comparison to Enterprise HR Platforms

| Capability | Commercial Tools | This Pipeline |
|---------|----------------|---------------|
| Pipeline transparency | ❌ | ✅ |
| Custom data integration | ❌ | ✅ |
| Open-source & reproducible | ❌ | ✅ |
| Distributed processing | ✅ | ✅ |
| Cloud portability | Limited | Full |

This project demonstrates how enterprise HR analytics systems can be **engineered from first principles** using open-source tools.

---

## Future Enhancements

- Cloud deployment (AWS EMR / Databricks)
- Parquet / Delta Lake storage
- Streaming ingestion (Kafka)
- Attrition prediction models
- Workflow orchestration (Airflow)

---

## Skills Demonstrated

- Distributed data processing with Apache Spark
- ETL pipeline design
- Big data joins and aggregations
- Schema harmonisation
- Feature engineering for analytics
- Performance optimisation
- Cloud-ready data architecture

---

## Report & Documentation

A detailed technical and academic report accompanies this repository and includes:
- Full pipeline screenshots
- Transformation walkthroughs
- Performance discussion
- Academic references
